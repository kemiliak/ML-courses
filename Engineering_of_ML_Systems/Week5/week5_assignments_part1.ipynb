{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55e65fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73ed3822dfef5b4f818bb564c13d0e5e",
     "grade": false,
     "grade_id": "cell-8d3d982d87fa93d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week5 Assignments (part1)\n",
    "\n",
    "**Please use the \"mlops_eng2\" Conda environment for this week's assignments.**\n",
    "\n",
    "In this week's assignments, you will build and run a KFP pipeline that trains and deploys a LightGBM regression model to predict public bike sharing demand. The assignments have two parts. In the first part (this notebook), You will create some KFP components following. In the [second part](./week5_assignments_part2.ipynb) you will create a KFP pipeline using your KFP components. \n",
    "\n",
    "**Guidelines for submitting the assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. Otherwise your notebook may not pass the tests used in grading. \n",
    "- At the end of the second part of the assignments, you will compile your KFP pipeline and save it to a YAML file `pipeline.yaml`. \n",
    "- Please return the assignment notebooks (`week5_assignment-part1.ipynb` and  `week5_assignments_part2.ipynb`) and the `pipeline.yaml` file generated in the second part.\n",
    "- For some assignments, you'll also need to capture some screenshots. Please put your screenshots in a PDF and also submit the PDF along with the other files mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea040a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d5989ef7c1627e1da54cac0ae5a93cb",
     "grade": false,
     "grade_id": "cell-c9216c282253ea89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp import kubernetes\n",
    "from kfp.dsl import component, Input, Output, Dataset\n",
    "from kfp.compiler import Compiler\n",
    "from typing import NamedTuple, Dict, Any\n",
    "\n",
    "import mlflow\n",
    "import lightgbm\n",
    "from unittest.mock import create_autospec\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.send_requests import send_requests\n",
    "\n",
    "assert kfp.__version__ == \"2.0.1\", \"Incorrect version of kfp\"\n",
    "assert lightgbm.__version__ == \"3.3.5\", \"Incorrect version of lightgbm\"\n",
    "\n",
    "# Suppress logging and warnings when grade the notebook\n",
    "# This is just for the grading purpose and doesn't affect you\n",
    "if os.environ.get(\"NBGRADER_EXECUTION\") in [\"autograde\", \"validate\"]:\n",
    "    logging.getLogger(\"\").setLevel(logging.ERROR)\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        logger.setLevel(logging.ERROR)\n",
    "    mlflow.utils.logging_utils.disable_logging()\n",
    "    warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee373ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98cbc0d6726670f2192845c9102bf543",
     "grade": false,
     "grade_id": "cell-825cce8bcb84029a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Create KFP components (10 points)\n",
    "You will need to create five KFP components, each KFP component gives 2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf427987",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d550a161e48e716284ca5dd95f292a8",
     "grade": false,
     "grade_id": "cell-5fbc9f38c55dd7eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1a) Create a pull data component\n",
    "\n",
    "This KFP component:\n",
    "\n",
    "1) downloads the dataset (a CSV file) as a Pandas DataFrame from a URL given as an input,\n",
    "\n",
    "2) saves the DataFrame to an output of type Dataset so that the dataset can be used by other KFP components.\n",
    "\n",
    "The dataset can be found [here](https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv). It's the same bike sharing demand dataset used in some of the previous weeks. Below is the explanation of each columns in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| datetime    | hourly date + timestamp| object\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise is 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| casual      | number of non-registered user rentals initiated | integer\n",
    "| registered  | number of registered user rentals initiated     | integer\n",
    "| count       | number of total rentals                         | integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad48f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9504b0046a7acf05f9e983a1150d03b1",
     "grade": false,
     "grade_id": "cell-ce4c027e3dec1267",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas~=2.2.0\"],\n",
    ")\n",
    "def pull_data(url: str, data: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: Dataset URL\n",
    "        data: Output of type Dataset where the downloaded dataset is saved\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(url, sep=\",\")\n",
    "    df.to_csv(data.path, index=None)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039e784",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57f6cbf4aff8f326d287d1701b00aa1a",
     "grade": false,
     "grade_id": "cell-52ff59b20200a8e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems there isn't a simply way to directly test the KFP component created by the @component decorator. A workaround is to access the inner decorated Python function through the component attribute `python_func`. \n",
    "\n",
    "Let's test if your `pull_data` works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d745cfb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfee4051353c236eb0afc23a81f73f0e",
     "grade": false,
     "grade_id": "cell-a804fb514aa908dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Mock an output Dataset\n",
    "Path(\"raw_data.csv\").unlink(missing_ok=True)\n",
    "raw_dataset = create_autospec(Dataset, metadata=dict(), path=\"raw_data.csv\")\n",
    "open(\"raw_data.csv\", \"w\").close() # Just make sure we have the write permission to the file\n",
    "\n",
    "pull_data.python_func(\n",
    "    url=\"https://raw.githubusercontent.com/yumoL/mlops_eng_course_datasets/master/intro/bike-demanding/train_full.csv\", \n",
    "    data=raw_dataset)\n",
    "\n",
    "raw_df = pd.read_csv(\"raw_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c612f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8947a45bd84883e77d1447ada7c98bea",
     "grade": false,
     "grade_id": "cell-4bbcf7eb46d8f4b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "![](./images/raw-df.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778930be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e13b9641a4ecc8da5c1161334e08a515",
     "grade": true,
     "grade_id": "cell-51c1a11efbf8e565",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert raw_df.shape == (10886, 12), \"The shape of the DataFrame is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b40825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c82c331a35b792b3963a39fe61af647",
     "grade": false,
     "grade_id": "cell-ac7343415f23efa5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1b) Create a data preprocessing component\n",
    "\n",
    "This KFP component: \n",
    "\n",
    "1. reads a dataset from an input of type Dataset,\n",
    "\n",
    "1. converts the \"datetime\" column into Pandas datetime object,\n",
    "\n",
    "1. creates three more features (hour, day and month) from the \"datetime\" column,\n",
    "\n",
    "1. removes the \"datetime\", \"casual\" and \"registered\" columns,\n",
    "\n",
    "1. splits the dataset into a training and a test dataset, using the last 168 rows of the dataset as the test dataset. (The dataset contains data from two years and the data were generated on an hourly basis so we use the last seven days (24*7=168) as the test data.)\n",
    "\n",
    "1. further splits the training and test datasets into features and targets (the target column is named \"target\") and saves them into four separate outputs of type Dataset.\n",
    "\n",
    "The resulted feature datasets should have 11 columns: season, holiday, workingday, weather, temp, atemp, humidity, windspeed, casual, registered, hour, day, and month. The target datasets should have 1 column: count.\n",
    "\n",
    "The following links may be helpful:\n",
    "- [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.to_datetime.html)\n",
    "- [pandas.Series.dt.*](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.Series.dt.hour.html)\n",
    "- [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a89f67",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "202aa98295d527d6d35394cf71d6620b",
     "grade": false,
     "grade_id": "cell-a3c593a5b0f13726",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas~=2.2.0\"],\n",
    ")\n",
    "def preprocess_data(\n",
    "    data: Input[Dataset],\n",
    "    train_x_csv: Output[Dataset],\n",
    "    train_y_csv: Output[Dataset],\n",
    "    test_x_csv: Output[Dataset],\n",
    "    test_y_csv: Output[Dataset],\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: Input of type Dataset where the dataset is read from\n",
    "        train_x_csv: Output of type Dataset where the training features are saved\n",
    "        train_y_csv: Output of type Dataset where the training target is saved\n",
    "        test_x_csv: Output of type Dataset where the test features are saved\n",
    "        test_y_csv: Output of type Dataset where the test target is saved\n",
    "    \"\"\"\n",
    "    target = \"count\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(data.path)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"day\"] = df[\"datetime\"].dt.day\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df = df.drop([\"datetime\", \"casual\", \"registered\"], axis=1)\n",
    "    train_df = df.iloc[:-168]\n",
    "    test_df = df.iloc[-168:]\n",
    "    train_x = train_df.drop([target], axis=1)\n",
    "    train_y = train_df[[target]]\n",
    "    test_x = test_df.drop([target], axis=1)\n",
    "    test_y = test_df[[target]]\n",
    "    train_x.to_csv(train_x_csv.path, index=None)\n",
    "    train_y.to_csv(train_y_csv.path, index=None)\n",
    "    test_x.to_csv(test_x_csv.path, index=None)\n",
    "    test_y.to_csv(test_y_csv.path, index=None)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50974444",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10ffba0fd64714f1de9046373bfbc24f",
     "grade": false,
     "grade_id": "cell-676f5748f0056df5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for file in [\"train_x.csv\", \"train_y.csv\", \"test_x.csv\", \"test_y.csv\"]:\n",
    "    Path(file).unlink(missing_ok=True)\n",
    "train_x_csv = create_autospec(Dataset, metadata=dict(), path=\"train_x.csv\")\n",
    "train_y_csv = create_autospec(Dataset, metadata=dict(), path=\"train_y.csv\")\n",
    "test_x_csv = create_autospec(Dataset, metadata=dict(), path=\"test_x.csv\")\n",
    "test_y_csv = create_autospec(Dataset, metadata=dict(), path=\"test_y.csv\")\n",
    "\n",
    "for file in [train_x_csv, train_y_csv, test_x_csv, test_y_csv]:\n",
    "    open(file.path, \"w\").close() # Juts to make sure we can write to the file\n",
    "    \n",
    "preprocess_data.python_func(\n",
    "    data=raw_dataset,\n",
    "    train_x_csv=train_x_csv,\n",
    "    train_y_csv=train_y_csv,\n",
    "    test_x_csv=test_x_csv,\n",
    "    test_y_csv=test_y_csv,\n",
    ")\n",
    "\n",
    "train_x_df = pd.read_csv(\"train_x.csv\")\n",
    "train_y_df = pd.read_csv(\"train_y.csv\")\n",
    "test_x_df = pd.read_csv(\"test_x.csv\")\n",
    "test_y_df = pd.read_csv(\"test_y.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a12c8d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b50d4cc02afd3a2a9f170869c21439db",
     "grade": true,
     "grade_id": "cell-8f9ce8b3b18bb134",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert train_x_df.shape == (\n",
    "    10718,\n",
    "    11,\n",
    "), \"The shape of the train_x DataFrame is incorrect.\"\n",
    "assert train_y_df.shape == (\n",
    "    10718,\n",
    "    1,\n",
    "), \"The shape of the train_y DataFrame is incorrect.\"\n",
    "assert test_x_df.shape == (168, 11), \"The shape of the test_x DataFrame is incorrect.\"\n",
    "assert test_y_df.shape == (168, 1), \"The shape of the test_y DataFrame is incorrect.\"\n",
    "\n",
    "expected_feature_columns = [\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "    \"weather\",\n",
    "    \"temp\",\n",
    "    \"atemp\",\n",
    "    \"humidity\",\n",
    "    \"windspeed\",\n",
    "    \"hour\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "]\n",
    "expected_target_column = \"count\"\n",
    "assert set(train_x_df.columns) == set(\n",
    "    expected_feature_columns\n",
    "), \"The columns of the training feature DataFrame are incorrect.\"\n",
    "assert train_y_df.columns == [\n",
    "    expected_target_column\n",
    "], \"The column of the training target DataFrame is incorrect.\"\n",
    "assert set(test_x_df.columns) == set(\n",
    "    expected_feature_columns\n",
    "), \"The columns of the test feature DataFrame are incorrect.\"\n",
    "assert test_y_df.columns == [\n",
    "    expected_target_column\n",
    "], \"The column of the train target DataFrame is incorrect.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96040d0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7165f5efeeb93bcd16222629430f6c35",
     "grade": false,
     "grade_id": "cell-aba42e16b658f31b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1c) Create an HPO (hyperparameter optimization) component\n",
    "We're going to train a LightGBM regression model to predict the bike sharing demand. In this KFP component, you need to create a component that uses Optuna to perform hyperparameter optimization for the LightGBM model.\n",
    "\n",
    "This KFP component first reads the feature and target datasets of both training and test data from four inputs of type Dataset, respectively. Then it performs hyperparameter optimization for the LightGBM model using Optuna. (You can check week3 materials if you need a refresher on how to use Optuna). \n",
    "\n",
    "The target of the optimization is to minimize the MAE (mean absolute error) of the model when evaluating the model against the testing dataset. The hyperparameters to be tuned and their search ranges are shown below. The value of `random_state` is fixed. The hyperparameter values should be sampled in a linear domain if not separately specified. Please specify the hyperparameters in the same order as presented in the table. When define the search space for the hyperparameters, please use the names given in the \"Hyperparameter\" column in the table. \n",
    "\n",
    "| Hyperparameter    | Explanation                                                                 | type    | range                                                                    |\n",
    "|:-------------------|:-----------------------------------------------------------------------------|:---------|:--------------------------------------------------------------------------|\n",
    "| learning_rate     | The step size of the gradient descent. It controls how quickly the model fits and then overfits the training data.              | float   | [0.001, 0.1] (sampled from the logarithmic domain) |\n",
    "| colsample_bytree  | The percentage of features to use when training each tree.                | float   | [0.05, 0.5]                                                              |\n",
    "| num_leaves        | Max number of nodes in a single tree.                                       | integer | [2, 2^10]                                                                |\n",
    "| random_state      | The seed for random number generation for reproducibility.                                   | integer | given as an argument `random_seed` |\n",
    "\n",
    "When run the study:\n",
    "- Use [TPESampler](https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html) as the sampler for the hyperparameter sampling and use the value of the `random_seed` argument as the seed of the sampler. \n",
    "- The Optuna study perform `hpo_trials` (this is also an argument) trials.\n",
    "- You don't need to persist the study anywhere.\n",
    "\n",
    "Finally, the component should return a namedtuple with the following fields:\n",
    "- `hyperparams`: the best hyperparameters found by the optimization\n",
    "- `best_mae`: the best MAE found by the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36501cb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68ab01c9951f884e4577e107b096b4a3",
     "grade": false,
     "grade_id": "cell-d4e95ad68d509276",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\n",
    "        \"pandas~=2.2.0\",\n",
    "        \"numpy~=1.26.4\",\n",
    "        \"lightgbm==3.3.5\",\n",
    "        \"optuna==3.5.0\",\n",
    "        \"scikit-learn~=1.4.0\",\n",
    "    ],\n",
    ")\n",
    "def hpo(\n",
    "    train_x_csv: Input[Dataset],\n",
    "    train_y_csv: Input[Dataset],\n",
    "    test_x_csv: Input[Dataset],\n",
    "    test_y_csv: Input[Dataset],\n",
    "    hpo_trials: int = 2,\n",
    "    random_seed: int = 42\n",
    ") -> NamedTuple(\n",
    "    \"Output\",\n",
    "    [\n",
    "        (\"hyperparams\", Dict[str, Any]),\n",
    "        (\"best_mae\", float),\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_x_csv: Input where the training feature data is saved\n",
    "        train_y_csv: Input where the training target data is saved\n",
    "        test_x_csv: Input where the test feature data is saved\n",
    "        test_y_csv: Input where the test target data is saved\n",
    "        hpo_trials: The number of trials that the Optuna study should run\n",
    "        random_seed: The random seed used for model training and t TPESampler\n",
    "    Returns:\n",
    "        namedtuple(\"Output\", [\"hyperparams\", \"best_mae\"]) where hyperparams is the best hyperparameter combination found by the optimization \n",
    "        and best_mae is the best MAE found by the optimization. The hyperparams is a dictionary where the keys are the hyperparameter names \n",
    "        and values the hyperparameter values. It should also contain the random_state used in model training. \n",
    "        The returned namedtuple should be like:\n",
    "        Output(hyperparams={'learning_rate': ..., 'colsample_bytree': ..., 'num_leaves': ..., 'random_state': <random_seed>}, best_mae=...)  \n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1. Read the feature and target datasets \n",
    "    # 2. Define the objective function\n",
    "    # 3. Create an Optuna study and run it. Assign the study to the \"study\" variable, i.e., study=optuna.create_study(...)\n",
    "    ### START CODE HERE\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import optuna\n",
    "    import lightgbm\n",
    "    train_x = pd.read_csv(train_x_csv.path)\n",
    "    train_y = pd.read_csv(train_y_csv.path)\n",
    "    test_x = pd.read_csv(test_x_csv.path)\n",
    "    test_y = pd.read_csv(test_y_csv.path)\n",
    "\n",
    "    def objective_func(trial):\n",
    "        params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"random_state\": random_seed,\n",
    "        }\n",
    "        model = lightgbm.LGBMRegressor(**params)\n",
    "        model.fit(train_x, train_y)\n",
    "        preds = model.predict(test_x)\n",
    "        mae = mean_absolute_error(test_y, preds)\n",
    "        return mae\n",
    "    \n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=random_seed))\n",
    "    study.optimize(objective_func, n_trials=hpo_trials)\n",
    "    ### END CODE HERE\n",
    "    \n",
    "    # Insert the random_state into the hyperparams when prepare the output namedtuple\n",
    "    hyperparams = {key: value for key, value in study.best_params.items()}\n",
    "    hyperparams[\"random_state\"] = random_seed\n",
    "\n",
    "    # TODO: Construct and return the namedtuple\n",
    "    ### START CODE HERE\n",
    "    Output = NamedTuple(\"Output\", [(\"hyperparams\", Dict[str, Any]), (\"best_mae\", float)])\n",
    "    return Output(hyperparams=hyperparams, best_mae=study.best_value)\n",
    "    ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad07ece6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eba5fcaf24b3b829e651bdfc21205fc7",
     "grade": false,
     "grade_id": "cell-ef331ed64e2bf81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 10:11:00,190] A new study created in memory with name: no-name-8b30c356-7fae-4ddb-8d38-92b218e0f3fd\n",
      "[I 2024-12-05 10:11:01,111] Trial 0 finished with value: 91.7245589134533 and parameters: {'learning_rate': 0.005611516415334507, 'colsample_bytree': 0.9753571532049581, 'num_leaves': 750}. Best is trial 0 with value: 91.7245589134533.\n",
      "[I 2024-12-05 10:11:01,463] Trial 1 finished with value: 80.8831514088002 and parameters: {'learning_rate': 0.015751320499779727, 'colsample_bytree': 0.5780093202212182, 'num_leaves': 161}. Best is trial 1 with value: 80.8831514088002.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output returned by the HPO component is Output(hyperparams={'learning_rate': 0.015751320499779727, 'colsample_bytree': 0.5780093202212182, 'num_leaves': 161, 'random_state': 42}, best_mae=80.8831514088002)\n"
     ]
    }
   ],
   "source": [
    "hpo_output = hpo.python_func(\n",
    "    train_x_csv=train_x_csv, \n",
    "    train_y_csv=train_y_csv, \n",
    "    test_x_csv=test_x_csv, \n",
    "    test_y_csv=test_y_csv, \n",
    "    hpo_trials=2, \n",
    "    random_seed=42\n",
    ")\n",
    "print(f\"The output returned by the HPO component is {hpo_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e87280",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "111979d2a8a979dade55976060120fe9",
     "grade": true,
     "grade_id": "cell-6c3541876858ff5c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = hpo_output.hyperparams\n",
    "for param_name in [\"learning_rate\", \"colsample_bytree\", \"num_leaves\", \"random_state\"]:\n",
    "    assert param_name in hyperparams, f\"{param_name} is not in the hyperparameters\"\n",
    "assert hpo_output.best_mae < 81, \"Too large MAE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de5e85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dccc8e3b23a01b4943094004e1385df9",
     "grade": false,
     "grade_id": "cell-9b371d1736a0dc03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "Output(hyperparams={'learning_rate': 0.015751320499779727, 'colsample_bytree': 0.5780093202212182, 'num_leaves': 161, 'random_state': 42}, best_mae=80.8831514088002)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076ef62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57d240c283fc3fc17bdd5f7d46d01f2d",
     "grade": false,
     "grade_id": "cell-c7258cf7b666516e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1d) Create a train component\n",
    "\n",
    "This component \n",
    "\n",
    "1) loads the feature and target datasets of both training and test data from four inputs of type Dataset, respectively,\n",
    "\n",
    "2) uses the training dataset and the given hyperparameters to train a [LightGBM regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html). The hyperparameters are given in a dictionary, e.g., `{\"num_leaves\": 1023, \"learning_rate\": 0.05, ...}`. In the dictionary, each key is an argument name accepted by the LGBMRegressor class and the value is the value assigned to the corresponding argument,\n",
    "\n",
    "3) evaluates the trained model against the testing dataset, the evaluation metrics to be used are **root mean squared error (RMSE), mean absolute error (MAE), r2_score**,\n",
    "\n",
    "4) logs the used hyperparameters and evaluation metrics to an MLflow Run. The following information is given as the component inputs: the name of the MLflow Experiment under which the MLflow Run should be stored, the URIs of the MLflow service and the artifact store,\n",
    "\n",
    "5) registers the model to MLflow, the artifact path relative to the MLflow Run is also given as an input,\n",
    "6) returns the (absolute) S3 URI of the saved model in Mlflow's artifact store, e.g., `s3://mlflow/<mlflow-experiment-id>/<mlflow-run-id>/artifacts/bike-demand`.\n",
    "\n",
    "**Notes**:\n",
    "- When logging hyperparameters, please use the keys of the hyperparameter dictionary as the parameter names.\n",
    "- When logging metrics, please use **\"rmse\", \"mae\", \"r2\"** as the metric names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd0d3e8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b89157f02b733a2c89672850275e2ddc",
     "grade": false,
     "grade_id": "cell-d66d96e115368fc0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"pandas~=2.2.0\", \"numpy~=1.26.4\", \"lightgbm~=3.3.5\", \"scikit-learn~=1.4.0\", \"mlflow==2.9.2\", \"boto3~=1.34.40\"],\n",
    ")\n",
    "def train(\n",
    "    train_x_csv: Input[Dataset],\n",
    "    train_y_csv: Input[Dataset],\n",
    "    test_x_csv: Input[Dataset],\n",
    "    test_y_csv: Input[Dataset],\n",
    "    mlflow_experiment_name: str,\n",
    "    mlflow_tracking_uri: str,\n",
    "    mlflow_s3_endpoint_url: str,\n",
    "    model_artifact_path: str,\n",
    "    hyperparams: Dict[str, Any],\n",
    ") -> str: \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_x_csv: Input where the training feature data is saved\n",
    "        train_y_csv: Input where the training target data is saved\n",
    "        test_x_csv: Input where the test feature data is saved\n",
    "        test_y_csv: Input where the test target data is saved\n",
    "        mlflow_experiment_name: Name of the MLflow experiment\n",
    "        mlflow_tracking_uri: URI of MLflow's tracking server\n",
    "        mlflow_s3_endpoint_url: URL of MLflow's artifact store\n",
    "        model_artifact_path: The path where the artifacts of the model are stored in MLflow's artifact store. It's relative to the MLflow Run.\n",
    "        hyperparams: Hyperparameters that need to be configured. The hyperparameters will be passed as a dictionary like {\"num_leaves\": 1023, \"learning_rate\": 0.05}\n",
    "    \n",
    "    Returns: \n",
    "        The S3 URI of the saved model in Mlflow's artifact store, e.g., s3://mlflow/13/e5559bc.../artifacts/bike-demand\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import logging\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import mlflow\n",
    "    import pandas as pd\n",
    "    import lightgbm\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def eval_metrics(actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2\n",
    "    \n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = mlflow_s3_endpoint_url\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "    train_x = pd.read_csv(train_x_csv.path)\n",
    "    train_y = pd.read_csv(train_y_csv.path)\n",
    "    test_x = pd.read_csv(test_x_csv.path)\n",
    "    test_y = pd.read_csv(test_y_csv.path)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id\n",
    "        logger.info(f\"Run ID: {run_id}\")\n",
    "        model = lightgbm.LGBMRegressor(**hyperparams)\n",
    "\n",
    "        logger.info(\"Fitting model...\")\n",
    "        model.fit(train_x, train_y)\n",
    "       \n",
    "        logger.info(\"Predicting...\")\n",
    "        predicted_qualities = model.predict(test_x)\n",
    "        rmse, mae, r2 = eval_metrics(test_y, predicted_qualities)\n",
    "        logger.info(f\"LightGBM model with hyperparameters: {hyperparams}\")\n",
    "        logger.info(f\"RMSE: {rmse}\")\n",
    "        logger.info(f\"MAE: {mae}\")\n",
    "        logger.info(f\"R2: {r2}\")\n",
    "\n",
    "        logger.info(\"Logging parameters and metrics to MLflow\")\n",
    "        for key, value in hyperparams.items():\n",
    "            mlflow.log_param(key, value)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "        logger.info(\"Logging trained model\")\n",
    "        mlflow.lightgbm.log_model(\n",
    "            lgb_model=model,\n",
    "            artifact_path=model_artifact_path,\n",
    "            registered_model_name=\"Week5LgbmBikeDemand\"\n",
    "        )\n",
    "        \n",
    "        storage_uri = mlflow.get_artifact_uri(model_artifact_path) \n",
    "        return storage_uri\n",
    "    ### END CODE HERE\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7026f7e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cea6ffed82346a991b06837f02328226",
     "grade": false,
     "grade_id": "cell-c45b523602322349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Run ID: b44222118cad4b2fba9bd135d5012156\n",
      "INFO:__main__:Fitting model...\n",
      "INFO:__main__:Predicting...\n",
      "INFO:__main__:LightGBM model with hyperparameters: {'learning_rate': 0.015751320499779727, 'colsample_bytree': 0.5780093202212182, 'num_leaves': 161, 'random_state': 42}\n",
      "INFO:__main__:RMSE: 110.44633998550202\n",
      "INFO:__main__:MAE: 80.8831514088002\n",
      "INFO:__main__:R2: 0.5756169522158694\n",
      "INFO:__main__:Logging parameters and metrics to MLflow\n",
      "INFO:__main__:Logging trained model\n",
      "/home/kaisaeko/anaconda3/envs/mlops_eng2/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "Registered model 'Week5LgbmBikeDemand' already exists. Creating a new version of this model...\n",
      "2024/12/05 10:11:05 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week5LgbmBikeDemand, version 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output returned by the Train component is s3://mlflow/8/b44222118cad4b2fba9bd135d5012156/artifacts/test-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '16' of model 'Week5LgbmBikeDemand'.\n"
     ]
    }
   ],
   "source": [
    "# These are needed for testing the function outside the MLOps platform\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "train_output = train.python_func(\n",
    "    train_x_csv=train_x_csv,\n",
    "    train_y_csv=train_y_csv,\n",
    "    test_x_csv=test_x_csv,\n",
    "    test_y_csv=test_y_csv,\n",
    "    mlflow_experiment_name=\"test\",\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    mlflow_s3_endpoint_url=\"http://mlflow-minio.local\",\n",
    "    model_artifact_path=\"test-model\",\n",
    "    hyperparams=hpo_output.hyperparams,\n",
    ")\n",
    "\n",
    "print(f\"The output returned by the Train component is {train_output}\")\n",
    "\n",
    "# Expected output:\n",
    "# The output returned by the Train component is s3://mlflow/<mlflow-experiment-id>/<mlflow-run-id>/artifacts/test-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f73c406",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b34b322f8365779e5b20b616f126353",
     "grade": true,
     "grade_id": "cell-c781d64863765965",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the \"train\" function\n",
    "import mlflow\n",
    "mlflow_run_id = train_output.split(\"/\")[-3]\n",
    "mlflow_client = mlflow.MlflowClient()\n",
    "mlflow_run = mlflow_client.get_run(mlflow_run_id)\n",
    "\n",
    "# Check if the logged params and metrics are correct\n",
    "assert set([\"mae\", \"r2\", \"rmse\"]) == set(mlflow_run.data.metrics.keys()), \"The metrics are not logged correctly.\"\n",
    "assert set([\"colsample_bytree\", \"learning_rate\", \"num_leaves\", \"random_state\"]) == set(mlflow_run.data.params.keys()), \"The hyperparameters are not logged correctly.\"\n",
    "\n",
    "# Check if a model is uploaded\n",
    "model_versions = mlflow_client.search_model_versions(filter_string=f\"run_id='{mlflow_run_id}'\")\n",
    "assert len(model_versions) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e28687",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d908986752921cf572e1cb44f70bde0f",
     "grade": false,
     "grade_id": "cell-a3ecd5cc808b5028",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "At [http://mlflow-server.local](http://mlflow-server.local), you should see an MLflow Experiment named \"test\" and there is an MLflow Run under the \"test\" experiment. You should also see a registered model associated to the MLflow Run:\n",
    "\n",
    "<img src=\"./images/mlflow-test-run.png\" width=\"1000\" />\n",
    "<img src=\"./images/mlflow-test-model.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aaff4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "461a10430af3b61f7b206916f3d216ad",
     "grade": false,
     "grade_id": "cell-8962d9335ae96981",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1e) Create a deploy model component\n",
    "\n",
    "This component uses KServe Python SDK to deploy the trained model to KServe in the \"kserve-inference\" namespace. The component should create a new inference service or update an existing one. In the tutorial, you may notice that the component used for model deployment may be completed though the deployed inference service is not yet ready. Here, **the component should remain running until the status of the deployed inference service is ready or a timeout of 6 minutes is reached.**\n",
    "\n",
    "The name of the inference service and the S3 URI of the model are given as inputs.\n",
    "\n",
    "**Hint**: \n",
    "- Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model saved by MLflow.\n",
    "- [kserve.wait_isvc_ready](https://kserve.github.io/website/0.10/sdk_docs/docs/KServeClient/#wait_isvc_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7862cce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43e24ede65848f7f883b2eedfc5e5409",
     "grade": false,
     "grade_id": "cell-7f76c1a21f272626",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"kserve==0.11.2\"],\n",
    ")\n",
    "def deploy_model(model_name: str, storage_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        storage_uri: the URI of the saved model in MLflow's artifact store\n",
    "    \"\"\"\n",
    "    from kubernetes import client\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1ModelSpec\n",
    "    from kserve import V1beta1ModelFormat\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name = \"kserve-sa\"\n",
    "    api_version = constants.KSERVE_V1BETA1\n",
    "    logger.info(f\"MODEL URI: {storage_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=storage_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                               kind=constants.KSERVE_KIND,\n",
    "                               metadata=client.V1ObjectMeta(\n",
    "                                   name=model_name, namespace=namespace),\n",
    "                               spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       model=modelspec,\n",
    "                                       service_account_name=service_account_name))\n",
    "                               )\n",
    "    \n",
    "    kserve = KServeClient()\n",
    "    try:\n",
    "        kserve.create(isvc)\n",
    "    except:\n",
    "        kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)\n",
    "    try:\n",
    "        kserve.wait_isvc_ready(name=model_name, namespace=namespace, timeout_seconds=360)\n",
    "    except:\n",
    "        TimeoutError\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc90606a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8ad7bd57c5911017ec66d7383ce95fb",
     "grade": false,
     "grade_id": "cell-e347c086efdfc712",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:MODEL URI: s3://mlflow/8/b44222118cad4b2fba9bd135d5012156/artifacts/test-model\n"
     ]
    }
   ],
   "source": [
    "# This function needs some time (<5min) to finish because it needs to wait for the inference service to be ready\n",
    "deploy_model.python_func(model_name=\"test-bike-demand\", storage_uri=train_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5843b55",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f64fdd14304aeeba3f8b302d8007dc90",
     "grade": true,
     "grade_id": "cell-93713901932cad4c",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'test-bike-demand', 'id': 'b2decd05-213a-418b-9b58-a4e5fbe1fb37', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [66.75263617512559, 64.23432649358075]}]}\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the inference service\n",
    "# The inference service should be immediately read when the deploy_model component is executed\n",
    "response = send_requests(isvc_name=\"test-bike-demand\")\n",
    "print(response.json())\n",
    "\n",
    "# Expected output:\n",
    "# {\"model_name\":\"test-bike-demand\",\"id\":\"13791e9c-3eb9-41b1-98c4-8f7da9f08d60\",\"parameters\":{},\n",
    "# \"outputs\":[{\"name\":\"output-1\",\"shape\":[2,1],\"datatype\":\"FP64\",\"data\":[35.894812901164,31.72387585260099]}]}\n",
    "\n",
    "assert response.json()[\"outputs\"][0][\"shape\"] == [2, 1], \"The inference service doesn't seem to be deployed correctly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2da6493",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20fb23b91e554552ff403d3059a151d1",
     "grade": false,
     "grade_id": "cell-2c0d2a9cb17083c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Run ID: add332f242d1479cbc735ffe2857d9e8\n",
      "INFO:__main__:Fitting model...\n",
      "INFO:__main__:Predicting...\n",
      "INFO:__main__:LightGBM model with hyperparameters: {'learning_rate': 0.1, 'colsample_bytree': 0.8, 'num_leaves': 100, 'random_state': 42}\n",
      "INFO:__main__:RMSE: 75.58283783525499\n",
      "INFO:__main__:MAE: 53.92626421925895\n",
      "INFO:__main__:R2: 0.8012524617586863\n",
      "INFO:__main__:Logging parameters and metrics to MLflow\n",
      "INFO:__main__:Logging trained model\n",
      "Registered model 'Week5LgbmBikeDemand' already exists. Creating a new version of this model...\n",
      "2024/12/05 10:11:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Week5LgbmBikeDemand, version 17\n",
      "Created version '17' of model 'Week5LgbmBikeDemand'.\n",
      "INFO:__main__:MODEL URI: s3://mlflow/8/add332f242d1479cbc735ffe2857d9e8/artifacts/test-model\n"
     ]
    }
   ],
   "source": [
    "# Train another model\n",
    "train_output2 = train.python_func(\n",
    "    train_x_csv=train_x_csv,\n",
    "    train_y_csv=train_y_csv,\n",
    "    test_x_csv=test_x_csv,\n",
    "    test_y_csv=test_y_csv,\n",
    "    mlflow_experiment_name=\"test\",\n",
    "    mlflow_tracking_uri=\"http://mlflow-server.local\",\n",
    "    mlflow_s3_endpoint_url=\"http://mlflow-minio.local\",\n",
    "    model_artifact_path=\"test-model\",\n",
    "    hyperparams={\"learning_rate\": 0.1, \"colsample_bytree\": 0.8, \"num_leaves\": 100, \"random_state\": 42},\n",
    ")\n",
    "\n",
    "deploy_model.python_func(model_name=\"test-bike-demand\", storage_uri=train_output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dd0927b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06b031eccafa24393442099abbbdf6a8",
     "grade": true,
     "grade_id": "cell-71dbf67d1b69c643",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Send a request to the inference service\n",
    "# The inference service should be immediately read when the deploy_model component is executed\n",
    "response2 = send_requests(isvc_name=\"test-bike-demand\")\n",
    "assert (\n",
    "    response2.json()[\"outputs\"][0][\"data\"] != response.json()[\"outputs\"][0][\"data\"]\n",
    "), \"The predictions should be different from the ones made by the previous inference service.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87dce2f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f973c09805ad7859d1bf678105e2ff7",
     "grade": false,
     "grade_id": "cell-38a89d62924d1f73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"test-bike-demand\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the testing inference service\n",
    "!kubectl -n kserve-inference delete isvc test-bike-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52fc443b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0c26c4f2cc3508fffbf7b91f178b57",
     "grade": false,
     "grade_id": "cell-5280e0fb43a617fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with ID add332f242d1479cbc735ffe2857d9e8 has been permanently deleted.\n",
      "Run with ID b44222118cad4b2fba9bd135d5012156 has been permanently deleted.\n"
     ]
    }
   ],
   "source": [
    "# Clean up by deleting the MLflow runs and models created in the test\n",
    "import subprocess\n",
    "mlflow_client = mlflow.MlflowClient()\n",
    "\n",
    "mlflow_exp = mlflow_client.get_experiment_by_name(\"test\")\n",
    "mlflow_runs = mlflow_client.search_runs(experiment_ids=[mlflow_exp.experiment_id])\n",
    "for mlflow_run in mlflow_runs:\n",
    "    model_versions = mlflow_client.search_model_versions(\n",
    "        f\"run_id='{mlflow_run.info.run_id}'\"\n",
    "    )\n",
    "    # Delete the registered model corresponding to the run if any\n",
    "    if len(model_versions) > 0:\n",
    "        mv = model_versions[0]\n",
    "        mlflow_client.delete_model_version(mv.name, mv.version)\n",
    "    # Permanently delete the run\n",
    "    mlflow_client.delete_run(mlflow_run.info.run_id)\n",
    "    subprocess.run([\"utils/delete_mlflow_run.sh\", mlflow_run.info.run_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b72dbc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "432015f439e330c8e4840172adcb8f6a",
     "grade": false,
     "grade_id": "cell-d6af90da8d8a0b90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's compile the KFP components you created and save them to YAML files located in the `./components` directory. The names of the YAML files should be same as your component function names. For example, the YAML file for the `pull_data` component is named `pull_data.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d648db0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f6acf2e4199c4dd227a440b21a68282",
     "grade": false,
     "grade_id": "cell-35a9406cf9f21d4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove the existing directory\n",
      "Generated components/pull_data.yaml\n",
      "Generated components/preprocess_data.yaml\n",
      "Generated components/hpo.yaml\n",
      "Generated components/train.yaml\n",
      "Generated components/deploy_model.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create the component folder\n",
    "component_dir_name = \"components\"\n",
    "if os.path.exists(component_dir_name):\n",
    "    print(\"Remove the existing directory\")\n",
    "    shutil.rmtree(component_dir_name)\n",
    "os.mkdir(component_dir_name)\n",
    "\n",
    "kfp_compiler = Compiler()\n",
    "for func_name in [\"pull_data\", \"preprocess_data\", \"hpo\", \"train\", \"deploy_model\"]:\n",
    "    kfp_compiler.compile(eval(func_name), os.path.join(component_dir_name, f\"{func_name}.yaml\"))\n",
    "    print(f\"Generated {os.path.join(component_dir_name, f'{func_name}.yaml')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc2f75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be6b6da3828823b9b38b5bb08304ae91",
     "grade": false,
     "grade_id": "cell-ddca83d5350ab7c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can continue to the [second part](./week5_assignments_part2.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
