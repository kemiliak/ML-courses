{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week4 Assignments (part2)\n",
    "This is the second part of this week's assignments. **Please run this notebook using the `mlops_eng2` environment** (it should be created when you follow the tutorials).\n",
    "\n",
    "### Guidelines for doing Assignments 2-5\n",
    "- In 2a), you'll need to write some Python code, so please put your code between the `### START CODE HERE` and `### END CODE HERE` comments. Please **do not change any code other than those between the `### START CODE HERE` and `### END CODE HERE` comments**. \n",
    "- In other assignments, you'll need to complete some configurations in YAML files. In each YAML file, please write your configurations between the `### START CONF HERE` and `### END CONF HERE` comments. Again, please **do not change any text other than those between the `### START CONF HERE` and `### END CONF HERE` comments**. \n",
    "- You will use a command `kubectl -n kserve-inference get isvc <name-of-inference-service>` (or `kubectl -n kserve-inference get ig <name-of-inference-graph>`) a few times when running this notebook. This command checks whether your inference service (or inference graph) deployed to KServe is ready. It takes some time (up to a few minutes) for a inference service/graph to become ready, so you may need to run the same command a few times to follow the readiness of your inference service/graph. You can also use the \"-w\" option to continuously watch the status of the inference service/graph (`kubectl get isvc <name-of-inference-service> -n kserve-inference -w`) and then terminate the code cell when the inference service/graph is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Deploy a model to KServe (3 points)\n",
    "In this assignment, you need to deploy your LightGBM model for predicting bike sharing demand as an inference service to KServe. You can use the model you just trained before starting the first assignment. \n",
    "\n",
    "Similar to the tutorial, the deployed inference service should run in the \"kserve-inference\" namespace and the service account name containing the credentials for accessing the MinIO storage service is also \"kserve-sa\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kserve_utils import send_request\n",
    "from utils.common_utils import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the correct version of lightgbm\n",
    "import lightgbm\n",
    "assert lightgbm.__version__ == \"3.3.5\", \"Your lightgbm version is not 3.3.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Use Python SDK to deploy your LightGBM model\n",
    "Complete the `deploy_model` function that uses the KServe SDK to deploy your LightGBM model. If there is no model deployed, your function should create a new inference service; if there is an inference service existing, your function should be able to update it. \n",
    "\n",
    "**Hint**: Using the LightGBM server provided by KServe doesn't work because the model saved by MLflow is in the pickled format, which is different from the format supported by KServe's LightGBM server. You can check [here](https://github.com/kserve/kserve/issues/2483) on how to use KServe SDK to deploy a model uploaded to MLflow.\n",
    "\n",
    "After complete and run the next code cell, you should see the code in the code cell exported to a Python script named `part2_answer.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b2869e092e9e48cfa4ff7a81515aee2",
     "grade": false,
     "grade_id": "cell-0507bc7b34f2190c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting part2_answer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile part2_answer.py\n",
    "\n",
    "from kubernetes import client\n",
    "from kserve import KServeClient\n",
    "from kserve import constants\n",
    "from kserve import V1beta1InferenceService\n",
    "from kserve import V1beta1InferenceServiceSpec\n",
    "from kserve import V1beta1PredictorSpec\n",
    "from kserve import V1beta1ModelSpec\n",
    "from kserve import V1beta1ModelFormat\n",
    "\n",
    "def deploy_model(model_name: str, model_uri: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name: the name of the deployed inference service\n",
    "        model_uri: the S3 URI of the model saved in MLflow\n",
    "    \"\"\"\n",
    "    \n",
    "    namespace = \"kserve-inference\"\n",
    "    service_account_name=\"kserve-sa\"\n",
    "    kserve_version=\"v1beta1\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + kserve_version\n",
    "    \n",
    "    print(f\"MODEL URI: {model_uri}\")\n",
    "    \n",
    "    modelspec = V1beta1ModelSpec(\n",
    "        storage_uri=model_uri,\n",
    "        model_format=V1beta1ModelFormat(name=\"mlflow\"),\n",
    "        protocol_version=\"v2\"\n",
    "    )\n",
    "    \n",
    "    isvc = V1beta1InferenceService(\n",
    "        ### START CODE HERE\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "        name=model_name, namespace=namespace),\n",
    "        ### END CODE HERE\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                ### START CODE HERE\n",
    "                model=modelspec,\n",
    "                service_account_name=service_account_name\n",
    "                ### END CODE HERE\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    kserve = KServeClient()\n",
    "\n",
    "    ### START CODE HERE\n",
    "    try:\n",
    "        kserve.create(isvc)\n",
    "    except:\n",
    "        kserve.patch(name=model_name, inferenceservice=isvc, namespace=namespace)\n",
    "    ### END CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found, skip training and use the existing model s3://mlflow/6/a9607b4f726a4f43924e09356d044447/artifacts/lgbm-bike\n",
      "MODEL URI: s3://mlflow/6/a9607b4f726a4f43924e09356d044447/artifacts/lgbm-bike\n"
     ]
    }
   ],
   "source": [
    "from part2_answer import deploy_model\n",
    "\n",
    "model_name = \"bike-lgbm-2a\"\n",
    "\n",
    "params = {\"num_leaves\": 63, \"learning_rate\": 0.05, \"random_state\": 42}\n",
    "model_uri = train(model_type=\"lgbm\", model_params=params, freshness_tag=\"old\")\n",
    "\n",
    "# Test the deploy_model function\n",
    "deploy_model(model_name, model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION            AGE\n",
      "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-00001   24s\n"
     ]
    }
   ],
   "source": [
    "# Check if the \"bike-lgbm-2a\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                    AGE\n",
    "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-default-00001   72s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                       READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-2a-predictor-00001-deployment-6c74c7c84f-gm86z   2/2     Running   0          27s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-2a-predictor-default-00001-deployment-6499598b7-wc28j   2/2     Running   0          65s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm-2a', 'id': 'f33049e8-fb71-4b33-adbb-a570be23c33c', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-lgbm-2a',\n",
       " 'id': 'f33049e8-fb71-4b33-adbb-a570be23c33c',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [2, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'data': [51.00457318737209, 35.13687405851507]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a request to the inference service\n",
    "send_request(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "{'model_name': 'bike-lgbm-2a',\n",
    " 'id': 'eddb6d4b-e517-421a-8420-d02db301428b',\n",
    " 'parameters': {},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [2, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'data': [51.00457318737209, 35.13687405851507]}]}\n",
    "```\n",
    "**Note**: The id varies. The important point is that the response has the correct fields as shown in the above expected output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P.S.* KServe also uses MLServer to serve the models uploaded to the MLflow service, which means your inference service also uses the V2 inference protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's train another model with different hyperparameters and see if your `deploy_model` function can update the existing inference service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found, skip training and use the existing model s3://mlflow/6/78d7260bfc0d4c14a99d386673eabc19/artifacts/lgbm-bike\n",
      "MODEL URI: s3://mlflow/6/78d7260bfc0d4c14a99d386673eabc19/artifacts/lgbm-bike\n"
     ]
    }
   ],
   "source": [
    "new_params = {\"num_leaves\": 31, \"learning_rate\": 0.01, \"random_state\": 42}\n",
    "new_model_s3_uri = train(model_type=\"lgbm\", model_params=new_params, freshness_tag=\"new\")\n",
    "\n",
    "deploy_model(model_name, new_model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           URL                                                READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION            AGE\n",
      "bike-lgbm-2a   http://bike-lgbm-2a.kserve-inference.example.com   True           100                              bike-lgbm-2a-predictor-00002   96s\n"
     ]
    }
   ],
   "source": [
    "# Check if the updated inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm-2a', 'id': '134b733b-a96f-4e18-ab15-d3cb5806bbee', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [97.60649891558708, 94.67018085698945]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-lgbm-2a',\n",
       " 'id': '134b733b-a96f-4e18-ab15-d3cb5806bbee',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [2, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'data': [97.60649891558708, 94.67018085698945]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_request(model_name=model_name)\n",
    "\n",
    "# The output data should be different from the previous response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm-2a\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2a\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Use a YAML file to deploy the model\n",
    "Instead of using the KServe SDK, now you need to use a YAML file to deploy your LightGBM model again. Please complete the configuration in [manifests/bike-lgbm-basic.yaml](./manifests/bike-lgbm-basic.yaml). You can use whichever LightGBM model in this assignment. \n",
    "\n",
    "**Hint**: You can check from [this KServe doc](https://kserve.github.io/website/0.11/modelserving/v1beta1/mlflow/v2/#deploy-with-inferenceservice) on how to use a YAML manifest to deploy a model stored in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the LightGBM model for bike demand prediction as an inference service named \"bike-lgbm\"\n",
    "!kubectl apply -f manifests/bike-lgbm-basic.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION         AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-00001   31s\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the \"bike-lgbm\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   2m24s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-predictor-00001-deployment-5949755c5-d85wr   2/2     Running   0          32s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is one pod running for the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: \n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-9d7b87595-k9kpk   2/2     Running   0          70s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'bike-lgbm', 'id': '53286e42-e982-4a62-8a61-ae171e73d63a', 'parameters': {}, 'outputs': [{'name': 'output-1', 'shape': [2, 1], 'datatype': 'FP64', 'data': [51.00457318737209, 35.13687405851507]}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'bike-lgbm',\n",
       " 'id': '53286e42-e982-4a62-8a61-ae171e73d63a',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-1',\n",
       "   'shape': [2, 1],\n",
       "   'datatype': 'FP64',\n",
       "   'data': [51.00457318737209, 35.13687405851507]}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send some requests to the \"bike-lgbm\" inference service\n",
    "send_request(model_name=\"bike-lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "{'model_name': 'bike-lgbm',\n",
    " 'id': '85c9e931-0879-4f88-a84c-137063e35064',\n",
    " 'parameters': {},\n",
    " 'outputs': [{'name': 'output-1',\n",
    "   'shape': [2, 1],\n",
    "   'datatype': 'FP64',\n",
    "   'data': [51.00457318737209, 35.13687405851507]}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please don't delete the \"bike-lgbm\" inference service, you will need it in Assignment3 later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Canary deployment in KServe (2 points)\n",
    "In this assignment, your task is to deploy the new model to KServe using the canary deployment strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to make sure there's already a \"bike-lgbm\" inference service running in KServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION         AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-00001   40s\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: \n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True           100                              bike-lgbm-predictor-default-00001   47m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to complete the configuration in [manifests/bike-lgbm-canary.yaml](./manifests/bike-lgbm-canary.yaml) to deploy a LightGBM model using canary deployment (Please use a different LightGBM model than the one you used in Assignment 2b). Your new inference service should receive **30%** of the user traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm configured\n"
     ]
    }
   ],
   "source": [
    "# Update the \"bike-lgbm\" inference service to use the new model\n",
    "!kubectl apply -f manifests/bike-lgbm-canary.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm configured\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION       LATESTREADYREVISION         AGE\n",
      "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True    70     30       bike-lgbm-predictor-00001   bike-lgbm-predictor-00002   88s\n"
     ]
    }
   ],
   "source": [
    "# Check that the traffic is splitted between the old and the new version\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   PREV   LATEST   PREVROLLEDOUTREVISION               LATESTREADYREVISION                 AGE\n",
    "bike-lgbm   http://bike-lgbm.kserve-inference.example.com   True    70     30       bike-lgbm-predictor-default-00001   bike-lgbm-predictor-default-00002   61m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                   READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-predictor-00001-deployment-5949755c5-d85wr   2/2     Running   0          90s\n",
      "bike-lgbm-predictor-00002-deployment-5f9898688-xdlk6   2/2     Running   0          47s\n"
     ]
    }
   ],
   "source": [
    "# Check there are two pods (one old and one new one) running \n",
    "!kubectl -n kserve-inference get pod -l serving.kserve.io/inferenceservice=bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                           READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-predictor-default-00001-deployment-cc96598f-rr2xz    2/2     Running   0          63m\n",
    "bike-lgbm-predictor-default-00002-deployment-6d9f5bbff-8mhn8   2/2     Running   0          3m36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Horizontal autoscaling (2 points)\n",
    "\n",
    "In this assignment, your task is to complete the configuration in [manifests/bike-lgbm-scale.yaml](./manifests/bike-lgbm-scale.yaml) to deploy your LightGBM model to KServe and configure the horizontal autoscaling feature for the deployed inference service. Specifically, the horizontal autoscaling of the inference service should satisfy the following requirements:\n",
    "1. The inference service should have ae least **2** pods running;\n",
    "2. The inference service can have at most **8** pods running when it's being scaled up;\n",
    "3. The target of the auto-scaling is that each pod running the inference service should receive **5** requests per second.\n",
    "\n",
    "You can use whichever LightGBM model you trained before. \n",
    "\n",
    "**Hint**: \"rps\" should be used as the scaling metric. \n",
    "\n",
    "*rps (requests per second) VS concurrency: These two metrics may look similar at the first glance. Both of them are metrics used to measure service performance. rps quantifies the number of requests a service can process within a specific time frame, often a second, whereas concurrency focuses on how many tasks a service can handle simultaneously.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n"
     ]
    }
   ],
   "source": [
    "# Deploy an inference service named \"bike-lgbm-scale\"\n",
    "!kubectl apply -f manifests/bike-lgbm-scale.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-scale created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION               AGE\n",
      "bike-lgbm-scale   http://bike-lgbm-scale.kserve-inference.example.com   True           100                              bike-lgbm-scale-predictor-00001   48s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"bike-lgbm-scale\" inference service is ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME              URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                       AGE\n",
    "bike-lgbm-scale   http://bike-lgbm-scale.kserve-inference.example.com   True           100                              bike-lgbm-scale-predictor-default-00001   90s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                          READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-scale-predictor-00001-deployment-5885cdd8b4-8m7dq   2/2     Running   0          49s\n",
      "bike-lgbm-scale-predictor-00001-deployment-5885cdd8b4-q9hvs   2/2     Running   0          50s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are two pods (replicas) running for the \"bike-lgbm-scale\" inference service\n",
    "# Please note that this command only check that your inference service is running, but it doesn't check if the scaling configuration is correct,\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferenceservice=bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcd67mr   2/2     Running   0          7m26s\n",
    "bike-lgbm-scale-predictor-default-00001-deployment-66df7bcjb6qx   2/2     Running   0          7m27s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Clean up by removing the \"bike-lgbm-scale\" inference service\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-scale\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Inference graph in KServe (3 points)\n",
    "\n",
    "## 5a) Inference graph for ensemble\n",
    "So far you already have two LightGBM models for predicting bike sharing demand. One was trained using the hyperparameters of {learning_rate=0.05, num_leaves=63} (denoted by Model A) and another {learning_rate=0.01, num_leaves=31} (denoted by Model B). \n",
    "\n",
    "You need to first complete the configuration in [manifests/bike-lgbm-graph.yaml](./manifests/bike-lgbm-graph.yaml) to deploy two inference services named \"bike-lgbm-1\" and \"bike-lgbm-2\". The \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services should serve Model A and B, respectively.\n",
    "\n",
    "Next, you need to complete [manifests/inference-graph1.yaml](./manifests/inference-graph1.yaml) to deploy an inference graph that includes one ensemble routing node. With this inference graph, a user will receive two predictions (one from each inference service) when they send a request.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-lgbm-1 created\n",
      "inferenceservice.serving.kserve.io/bike-lgbm-2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services\n",
    "!kubectl apply -f manifests/bike-lgbm-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-1 created\n",
    "inferenceservice.serving.kserve.io/bike-lgbm-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
      "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00001   40s\n",
      "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   40s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the two inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                   AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-default-00001   105m\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-default-00001   105m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-1-predictor-00001-deployment-84d864bc4b-tqcxb   2/2     Running   0          44s\n",
      "bike-lgbm-2-predictor-00001-deployment-7565f6c6c-nrg2f    2/2     Running   0          44s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are two pods running for the two inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                                              READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-default-00001-deployment-794547df56-48dhk   2/2     Running   0          109m\n",
    "bike-lgbm-2-predictor-default-00001-deployment-cf7b449b5-rjc2q    2/2     Running   0          109m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io/my-graph1 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the inference graph named \"my-graph1\"\n",
    "!kubectl apply -f manifests/inference-graph1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph1 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   AGE\n",
      "my-graph1   http://my-graph1.kserve-inference.example.com   True    10s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the \"my-graph1\" inference graph is ready\n",
    "!kubectl -n kserve-inference get ig my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph1   http://my-graph1.kserve-inference.example.com   True    102s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          READY   STATUS    RESTARTS   AGE\n",
      "my-graph1-00001-deployment-6955dff668-2b447   2/2     Running   0          12s\n"
     ]
    }
   ],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l serving.kserve.io/inferencegraph=my-graph1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph1-00001-deployment-7c4d7cfbf9-tr5tz   2/2     Running   0          2m7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's send a request to the \"my-graph1\" inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike-lgbm-1': {'id': 'cc5b0963-0522-4e46-9654-3a0f32c2386e', 'model_name': 'bike-lgbm-1', 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'output-1', 'shape': [2, 1]}], 'parameters': {}}, 'bike-lgbm-2': {'id': 'be47016c-0cbc-40b0-aa3c-cb061a1cc1fa', 'model_name': 'bike-lgbm-2', 'outputs': [{'data': [97.60649891558708, 94.67018085698945], 'datatype': 'FP64', 'name': 'output-1', 'shape': [2, 1]}], 'parameters': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bike-lgbm-1': {'id': 'cc5b0963-0522-4e46-9654-3a0f32c2386e',\n",
       "  'model_name': 'bike-lgbm-1',\n",
       "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
       "    'datatype': 'FP64',\n",
       "    'name': 'output-1',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}},\n",
       " 'bike-lgbm-2': {'id': 'be47016c-0cbc-40b0-aa3c-cb061a1cc1fa',\n",
       "  'model_name': 'bike-lgbm-2',\n",
       "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
       "    'datatype': 'FP64',\n",
       "    'name': 'output-1',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send a request\n",
    "from utils.kserve_utils import send_request\n",
    "\n",
    "send_request(to_ig=True, ig_name=\"my-graph1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output (i.e., the response) is expected to contain a prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '3f16b921-e18a-4b51-93c3-f084bf11d07c',\n",
    "  'model_name': 'bike-lgbm-1',\n",
    "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-lgbm-v2': {'id': 'ef1c1ec3-9880-467d-8773-1bf614d8e0bf',\n",
    "  'model_name': 'bike-lgbm-2',\n",
    "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Do not delete the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services. They're still needed in the next assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b) More complicated inference graph\n",
    "In this assignment, you need to deploy a more complex inference graph containing more than one routing node. \n",
    "\n",
    "First let's train two XGBoost models, denoted by Model C and Model D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found, skip training and use the existing model s3://mlflow/7/0f61bc4a11b041c8afc53075a9645401/artifacts/xgb-bike\n",
      "Model found, skip training and use the existing model s3://mlflow/7/c3a57d7ddc634b3cacbd0b395c47b2da/artifacts/xgb-bike\n",
      "First xgb model URI: s3://mlflow/7/0f61bc4a11b041c8afc53075a9645401/artifacts/xgb-bike\n",
      "Second xgb model URI: s3://mlflow/7/c3a57d7ddc634b3cacbd0b395c47b2da/artifacts/xgb-bike\n"
     ]
    }
   ],
   "source": [
    "old_xgb_model_s3_uri = train(\n",
    "    model_type=\"xgb\",\n",
    "    model_params={\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    freshness_tag=\"old\",\n",
    ")\n",
    "\n",
    "new_xgb_model_s3_uri = train(\n",
    "    model_type=\"xgb\",\n",
    "    model_params={\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"objective\": \"reg:absoluteerror\",\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    freshness_tag=\"new\",\n",
    ")\n",
    "\n",
    "print(\"First xgb model URI:\", old_xgb_model_s3_uri)\n",
    "print(\"Second xgb model URI:\", new_xgb_model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Assignment 5a, your tasks are to\n",
    "1. Complete [manifests/bike-xgb-graph.yaml](./manifests/bike-xgb-graph.yaml) to deploy two more inference services named \"bike-xgb-1\" and \"bike-xgb-2\" that serve Models C and D, respectively. \n",
    "2. Complete [manifests/inference-graph2.yaml](./manifests/inference-graph2.yaml) to deploy an inference graph containing two Ensemble routing nodes. \n",
    "The requests that will be sent to the inference graph look like:\n",
    "    ```python\n",
    "    {\n",
    "      'inputs': ...,\n",
    "      'modelType': 'lgbm'\n",
    "    }\n",
    "    ```\n",
    "    The inference graph should satisfy the following requirements:\n",
    "    - If there is a field named \"modelType\" in the request and its value is \"lgbm\", the request should be forwarded to an ensemble consisting of the \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services (These two inference services should be created in Assignment 5a). At this time, the user should receive one prediction from the \"bike-lgbm-1\" inference service and another prediction from the \"bike-lgbm-2\" inference service. \n",
    "    - If the value of \"modelType is \"xgb\", the request should be forwarded to another ensemble consisting of the \"bike-xgb-1\" and \"bike-xgb-2\" inference services. At this time, the user should receive one prediction from the \"bike-xgb-1\" inference service and another prediction from the \"bike-xgb-2\" inference service.\n",
    "    - Otherwise an error message should be returned, complaining that the request can't be processed. \n",
    "    \n",
    "    The behavior of the inference graph is illustrated in the figure below:\n",
    "\n",
    "    <img src=\"./images/complex-inference-graph.png\" width=600/>\n",
    "\n",
    "**Hints**:\n",
    "You may notice that you need to route requests from one routing node to another (instead of from a routing node to an inference service). Below is an example of configuring a routing node to forward requests to another routing node:\n",
    "```yaml\n",
    "...\n",
    "spec: \n",
    "  nodes: \n",
    "    # The first routing node\n",
    "    root: \n",
    "      routerType: ...\n",
    "      steps: \n",
    "      # This routing node forwards requests to the second routing node named \"ensembleNode\"\n",
    "      - nodeName: ensembleNode\n",
    "\n",
    "    # The second routing node\n",
    "    ensembleNode:\n",
    "      routerType: ...\n",
    "      steps:\n",
    "      ...\n",
    "```\n",
    "You can use `\"[@this].#(modelType==\\\"...\\\")\"` as the condition that determines which ensemble a request should be routed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/bike-xgb-1 created\n",
      "inferenceservice.serving.kserve.io/bike-xgb-2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the third inference service named \"bike-lgbm-3\"\n",
    "!kubectl apply -f manifests/bike-xgb-graph.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferenceservice.serving.kserve.io/bike-xgb-1 created\n",
    "inferenceservice.serving.kserve.io/bike-xgb-2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
      "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00001   2m\n",
      "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   2m\n",
      "bike-xgb-1    http://bike-xgb-1.kserve-inference.example.com    True           100                              bike-xgb-1-predictor-00001    28s\n",
      "bike-xgb-2    http://bike-xgb-2.kserve-inference.example.com    True           100                              bike-xgb-2-predictor-00001    27s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the all of the four inference services are ready\n",
    "!kubectl -n kserve-inference get isvc bike-lgbm-1 bike-lgbm-2 bike-xgb-1 bike-xgb-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME          URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION           AGE\n",
    "bike-lgbm-1   http://bike-lgbm-1.kserve-inference.example.com   True           100                              bike-lgbm-1-predictor-00001   9m21s\n",
    "bike-lgbm-2   http://bike-lgbm-2.kserve-inference.example.com   True           100                              bike-lgbm-2-predictor-00001   9m20s\n",
    "bike-xgb-1    http://bike-xgb-1.kserve-inference.example.com    True           100                              bike-xgb-1-predictor-00001    56s\n",
    "bike-xgb-2    http://bike-xgb-2.kserve-inference.example.com    True           100                              bike-xgb-2-predictor-00001    56s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
      "bike-lgbm-1-predictor-00001-deployment-84d864bc4b-tqcxb   2/2     Running   0          2m2s\n",
      "bike-lgbm-2-predictor-00001-deployment-7565f6c6c-nrg2f    2/2     Running   0          2m2s\n",
      "bike-xgb-1-predictor-00001-deployment-576c89f5c7-zm8xt    2/2     Running   0          30s\n",
      "bike-xgb-2-predictor-00001-deployment-5cd4fddc54-26kzk    2/2     Running   0          30s\n"
     ]
    }
   ],
   "source": [
    "# Make sure there are three pods running for the three inference services, respectively\n",
    "!kubectl -n kserve-inference get pods -l \"serving.kserve.io/inferenceservice in (bike-lgbm-1,bike-lgbm-2,bike-xgb-1, bike-xgb-2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                                      READY   STATUS    RESTARTS   AGE\n",
    "bike-lgbm-1-predictor-00001-deployment-68668c6667-5rb89   2/2     Running   0          9m48s\n",
    "bike-lgbm-2-predictor-00001-deployment-7d69db959f-bl75c   2/2     Running   0          9m47s\n",
    "bike-xgb-1-predictor-00001-deployment-5f7fdffbf8-l5q2p    2/2     Running   0          83s\n",
    "bike-xgb-2-predictor-00001-deployment-7797598c87-sxn49    2/2     Running   0          83s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io/my-graph2 created\n"
     ]
    }
   ],
   "source": [
    "# Deploy the second inference graph (\"my-graph2\")\n",
    "!kubectl apply -f manifests/inference-graph2.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "inferencegraph.serving.kserve.io/my-graph2 created\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME        URL                                             READY   AGE\n",
      "my-graph2   http://my-graph2.kserve-inference.example.com   True    14s\n"
     ]
    }
   ],
   "source": [
    "# Make sure the inference graph named \"my-graph2\" is ready\n",
    "!kubectl -n kserve-inference get ig my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "NAME        URL                                             READY   AGE\n",
    "my-graph2   http://my-graph2.kserve-inference.example.com   True    35s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          READY   STATUS    RESTARTS   AGE\n",
      "my-graph2-00001-deployment-856d6bf994-jt8dk   2/2     Running   0          17s\n"
     ]
    }
   ],
   "source": [
    "# Also make sure there is one pod running for the \"ensemble\" inference graph\n",
    "!kubectl -n kserve-inference get pods -l  serving.kserve.io/inferencegraph=my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "```text\n",
    "NAME                                          READY   STATUS    RESTARTS   AGE\n",
    "my-graph2-00001-deployment-557678dfbd-zglcg   2/2     Running   0          49s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike-lgbm-1': {'id': 'c739f00a-55d2-49b5-8b9e-eb20b644a0b9', 'model_name': 'bike-lgbm-1', 'outputs': [{'data': [51.00457318737209, 35.13687405851507], 'datatype': 'FP64', 'name': 'output-1', 'shape': [2, 1]}], 'parameters': {}}, 'bike-lgbm-2': {'id': '9905bf1b-079a-4d40-b439-86d602077f6a', 'model_name': 'bike-lgbm-2', 'outputs': [{'data': [97.60649891558708, 94.67018085698945], 'datatype': 'FP64', 'name': 'output-1', 'shape': [2, 1]}], 'parameters': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bike-lgbm-1': {'id': 'c739f00a-55d2-49b5-8b9e-eb20b644a0b9',\n",
       "  'model_name': 'bike-lgbm-1',\n",
       "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
       "    'datatype': 'FP64',\n",
       "    'name': 'output-1',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}},\n",
       " 'bike-lgbm-2': {'id': '9905bf1b-079a-4d40-b439-86d602077f6a',\n",
       "  'model_name': 'bike-lgbm-2',\n",
       "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
       "    'datatype': 'FP64',\n",
       "    'name': 'output-1',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send some requests\n",
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is expected to contain predictions from both \"bike-lgbm-1\" and \"bike-lgbm-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```text\n",
    "{'bike-lgbm-v1': {'id': '09be4efc-65f4-40ce-be91-e326bcf74ca5',\n",
    "  'model_name': 'bike-lgbm-1',\n",
    "  'outputs': [{'data': [51.00457318737209, 35.13687405851507],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-lgbm-v2': {'id': 'd3c53425-e426-4a79-8517-477994a7c49b',\n",
    "  'model_name': 'bike-lgbm-2',\n",
    "  'outputs': [{'data': [97.60649891558708, 94.67018085698945],\n",
    "    'datatype': 'FP64',\n",
    "    'name': 'output-1',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bike-xgb-1': {'id': '610b0226-3b37-4375-92a4-68ecfcd2e310', 'model_name': 'bike-xgb-1', 'outputs': [{'data': [112.01314544677734, 97.72420501708984], 'datatype': 'FP32', 'name': 'predict', 'shape': [2, 1]}], 'parameters': {}}, 'bike-xgb-2': {'id': '1f32a955-f88a-4fff-84a4-ebf7e23db812', 'model_name': 'bike-xgb-2', 'outputs': [{'data': [137.37649536132812, 131.63937377929688], 'datatype': 'FP32', 'name': 'predict', 'shape': [2, 1]}], 'parameters': {}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bike-xgb-1': {'id': '610b0226-3b37-4375-92a4-68ecfcd2e310',\n",
       "  'model_name': 'bike-xgb-1',\n",
       "  'outputs': [{'data': [112.01314544677734, 97.72420501708984],\n",
       "    'datatype': 'FP32',\n",
       "    'name': 'predict',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}},\n",
       " 'bike-xgb-2': {'id': '1f32a955-f88a-4fff-84a4-ebf7e23db812',\n",
       "  'model_name': 'bike-xgb-2',\n",
       "  'outputs': [{'data': [137.37649536132812, 131.63937377929688],\n",
       "    'datatype': 'FP32',\n",
       "    'name': 'predict',\n",
       "    'shape': [2, 1]}],\n",
       "  'parameters': {}}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response should only have predictions from the \"bike-xgb-1\" and \"bike-xgb-2\" inference services.\n",
    "\n",
    "Example output:\n",
    "```text\n",
    "{'bike-xgb-v1': {'id': '51e997c0-74e2-49b7-a273-3c489c848486',\n",
    "  'model_name': 'bike-xgb-1',\n",
    "  'outputs': [{'data': [112.01314544677734, 97.72420501708984],\n",
    "    'datatype': 'FP32',\n",
    "    'name': 'predict',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}},\n",
    " 'bike-xgb-v2': {'id': '71d65beb-c085-4787-8151-2a3f0fe6f504',\n",
    "  'model_name': 'bike-xgb-2',\n",
    "  'outputs': [{'data': [137.37649536132812, 131.63937377929688],\n",
    "    'datatype': 'FP32',\n",
    "    'name': 'predict',\n",
    "    'shape': [2, 1]}],\n",
    "  'parameters': {}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Failed to process request', 'cause': 'None of the routes matched with the switch condition'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'Failed to process request',\n",
       " 'cause': 'None of the routes matched with the switch condition'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "send_request(to_ig=True, ig_name=\"my-graph2\", model_type=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error message should be returned.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```text\n",
    "{'error': 'Failed to process request',\n",
    " 'cause': 'None of the routes matched with the switch condition'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-xgb-1\" deleted\n",
      "inferenceservice.serving.kserve.io \"bike-xgb-2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete all of the three inference services\n",
    "!kubectl -n kserve-inference delete isvc bike-lgbm-1 bike-lgbm-2 bike-xgb-1 bike-xgb-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-1\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-2\" deleted\n",
    "inferenceservice.serving.kserve.io \"bike-lgbm-3\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
      "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete all inference graphs\n",
    "!kubectl -n kserve-inference delete ig my-graph1 my-graph2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```text\n",
    "inferencegraph.serving.kserve.io \"my-graph1\" deleted\n",
    "inferencegraph.serving.kserve.io \"my-graph2\" deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up\n",
    "Please make sure you have the following files in your submission:\n",
    "- part1_answer.py and part2_answer.py\n",
    "- model-settings.json \n",
    "- all YAML files listed in the \"manifests\" directory\n",
    "\n",
    "When submit the files, please **do not** change the file names or put any of them in any sub-folder. The screenshot below shows an expected submission:\n",
    "\n",
    "<img src=\"./images/submission-example.png\" width=700/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
